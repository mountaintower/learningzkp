作者：MT， https://github.com/mountaintower/learningzkp

## 01 Greyhound

是基于lattice的PCS，基于laBRADOR 实现PCS。

## 02 内外双层承诺 (承诺多项式$f(X)$)

假设$f(X)=f_0 X^0 + f_1 X^1 + ... + f_{N-1} X^{N-1}$，将N个多项式系数切分成$r*m$ 矩阵，r*m=N，后面，我们可以将多项式系数矩阵表示成每个m维向量$\vec{f_i}$ 作为列向量的矩阵 。

现在承诺r条m维向量$\vec{f_0}, ..., \vec{f_{r-1}} \in \mathcal{R_q^m}$ ：

1）内层承诺：因为$\vec{f_i}$ 可能很长，不能直接承诺。故将每个$\vec{f_i}$ 分解为短向量$\vec{s_i} \in \mathcal{R_q^{m\delta}}$ (指m维向量$\vec{f_i}$ 中的每一维都拆成$\delta$ 份=>$(f_{i0}, f_{i1}, ..., f_{i (\delta-1)})$，$\delta$ 是分解基)，然后做Ajtai承诺得到$\vec{t_i}=A \vec{s_i} \in \mathcal{R_q^n}$， $A$ 是$n * mδ$ 矩阵。

2）外层承诺：将每个n维向量 $\vec{t_i}$ 分解为短向量$\vec{t'_i} \in \mathcal{R_q^{n\delta}}$，然后将r个$\vec{t'_i}$ 串联起来($\in \mathcal{R_q^{r n \delta}}$)，再次做Ajtai承诺得到$\vec{u}=B (\vec{t'_1}, ..., \vec{t'_n}) \in \mathcal{R_q^n}$， $B$ 是$n * rnδ$ 矩阵。

等于将多项式系数表示成了短向量$\vec{u}$。

分解的时候可以选择2进制，这样分解的结果只有0或1，向量小，容易处理。

## 03 二次型关系（两个线性映射的乘积，如$a F b=y$ ,参见第7课第6页）

上面的双层承诺已经把多项式系数压缩成一个短承诺 $\vec{u}$ ，现在证明：对于公开点x，二次型关系成立，并且不会泄露witness。

witness：$(\vec{s_i}, \vec{t_i})_{i \in [r]}$。
公开输入：$\vec{u}，\vec{a}，\vec{b}$，整数 $y$。

关系：

验证矩阵：$y=f(x)$ 求值；

验证$\vec{f_i}$正确分解成$\vec{s_i}$：$\vec{f_i}=G_m \vec{s_i}$；

验证$\vec{t_i}$ 是$\vec{s_i}$ 的承诺；

验证$\vec{t_i}$生成$u$：先分解成$\vec{t'_i}$，然后生成最终承诺。

## 04 简单版协议：证明上述关系

类似brakedown的方法：

1）P发左乘向量$\vec{w}$；

2）V发短向量挑战$\vec{c}$；

3）P发分解向量$\vec{t'_i}$，计算$\vec{z}$（即$\vec{c}$ 和$\vec{s}$ 的随机线性组合，$\vec{s}$ 与$\vec{f}$ 只差一个$G_m$，验证$\vec{c}$ 和$\vec{f}$ 的组合也是一样的）

## 05简单版协议

4）V验证：

y的计算正确，即第一行验证的前半部分；

$\vec{w}$ 的计算正确，即第一行验证的后半部分：挑战随机线性组合关系的计算正确（左边等于$\vec{a} \vec{f} \vec{c}$，右边等于$\vec{a} G_m \vec{s} \vec{c}= \vec{a} \vec{f} \vec{c}$）；

验证ajtai承诺正确，即第二行验证的前两个部分：先用分解向量$\vec{t'_i}$ 组合成inner承诺$\vec{t_i}$，然后验证$\vec{t_i}$ 与挑战$\vec{c}$ 组合是否等于线性组合$\vec{z}$ 的承诺；

验证outer 承诺，即第二行验证的最后一个部分：用inner承诺$\vec{t_i}$ 的承诺验证；

最后验证$\vec{t'_i}, \vec{z}$ 是短的。

## 06 减少证据大小：使用LaBRADOR关系

P发送的大小取决于： $\vec{w} \in \mathcal{R_q^r}，\vec{t'_i} \in \mathcal{R_q^{n \delta}}$（虽然大，与r和m无关），$\vec{z} \in \mathcal{R_q^m}$。

使用LaBRADOR关系描述第5页的验证关系，从而将代价减少到 $r=m=\sqrt N$。

## 07 减少证据大小：使用LaBRADOR关系

首先，第4页协议第1步：P不发送$\vec{w}$，而是发送$\vec{w}$分解后的$\vec{w'}$的承诺$\vec{v}$ ，以及第3步发送 $\vec{w'}$；

然后，第5页验证：要增加验证承诺$\vec{v} \overset{?}= Com(\vec{w'})$。

单纯地看，Proof大小还增加了分解后的$\vec{w'}$，但这里的问题是如何调用LaBRADOR。

使用本页的矩阵乘法：

第1行：验证承诺$\vec{v}$;

第2行：验证第5页outer 承诺$\vec{u}$；

第3行：验证第5页$\vec{w}$ 和$\vec{b}$ 左乘等于求值y （第5页第1行左边验证）；

第4行：验证第5页$\vec{w}$ 和$\vec{c}$ 随机线性组合等于$\vec{w}, G_m$  和$\vec{z}$ 线性组合（第5页第1行右边验证），即验证$\vec{w}$ 是正确的；

第5行：验证第5页$\vec{t_i}$ 的分解和ajtai承诺(见第3页对$\vec{s_i}$ 的承诺$\vec{t_i}$ )；

最后验证3个向量是短的。
若将矩阵公式精简成$E \vec{d}= \vec{f}$ ，那么 $\vec{d}$ 是witness，其它都是公开输入。这个关系完全可以用第6页LaBRADOR的公式第2项（$\Sigma<\Phi_i,s_j>$）检查。

所以Greyhound 等于把N维向量变为$\sqrt N$ 大小的向量，然后在上面实施LaBRADOR-IPA得到$O(log\sqrt N)=O(logN)$ 的Proof大小，验证时间是$O(\sqrt N)$ 的。

## 08 多项式承诺

最终要证明的是普通整数域$ {ℤ_q} $上的多项式$f(X)$求值（多项式的系数$f_i$ 是标量），而Greyhound的双层承诺是在环 $R_q = ℤ_q[X]/(X^m + 1)$ 定义的（多项式$f(X)$的r 个原始系数$\vec{f_i}$ 都是m维向量，见第2页第1行，所以需要被拉平成mr个${ℤ_q}$标量计算）。

即把在$R_q$ 上的$f(X)$ 求值证明扩展到${ℤ_q}$上：$f(x) = Σ_{i=0}^{mr-1} f_i x^i = y\ mod\ q$。

Greyhound 方案：

1）把r 个环元素系数$\vec{f_i}$ 展开成 m·r个$ℤ_q$标量（即把环里的每个多项式系数 $\vec{f_i}$ 拆成 m个整数系数），系数$\vec{f_i} $ 的展开见PPT，按 m×r 矩阵展开）：$f(x)= [1,x,x^2,…,x^{m-1}][\vec{f_0},\vec{f_1},...\vec{f_{r-1}}][1,x^m,x^{2m},…,x^{(r-1)m}]^T$。

这里的$f(x) $分解的3项中的第1个（长度为m）和第3个（长度为r）与x相关的向量，相当于第3页的向量a和b，做外积展平： $a ⊗ b=（1，x^1, ... x^{mr-1})$。所以全部乘起来就是$f(x)$的取值，这样可以压缩协议传输的通信量。

2）用$ℤ_q$上的内积完成最终的Σ-check 求值检查：

已知 $f(x)=⟨ (f_0,…,f_{mr-1}), (1,x,x²,…,x^{mr-1}) ⟩ $， 实际上是对公开点z，求解$<\vec{f},\vec{z}>=\Sigma_{i \in [0,mr-1]} f_i z_i$。 采用LaBRADOR的$ct(<\sigma(\vec{a})，\vec{a})>$技巧(见LaBRADOR讲课PPT第2页)，就能将上述向量内积转换为$ct(<\sigma(\vec{f})，\vec{z})>$ （交叉项都被丢弃了）。

然后用LaBRADOR-IPA就可以一次性递归证明内积了，代价是O(logN) 。

## 09 HyperWolf：从单变量->多变量线性

目标：把 Greyhound 的二次型关系证明从二维矩阵压缩推广到 k 维超立方体，进一步降低通信与验证复杂度。

核心思想：

1）将长向量 f（长度 N）视为 k 维超立方体网格（每维大小 N^{1/k}）。

2）用 k 维分段幂向量 代替 Greyhound 的二维 a、b 向量：通信/验证复杂度降至 $O(k N^{1/k})$。

3）取 k = log N 时，复杂度变为 O(log N)，无需再跑 LaBRADOR，协议自身即递归 Σ-check。

## 10 -11 HyperWolf举例（假设有8个$f_i$系数，立方体）

证明这里的$f(x)$取值（3个变量$x_i$），将x向量拆成$x_1, x_2$和$x_3$两部分

所以reduce成的第9页最后一行图的关系（蓝色部分乘积公开g_i，绿色部分乘积公开y_g）等同第10页最后一行

## 12 HyperWolf举例

这里实际上是Greyhound了，V最后验证的等式类似第5页Grey第一行后半部分验证。

## 答疑

关于knowledge soundness：greyhound PCS 调用了LaBRADOR的knowledge soundness
仍然使用rewind技术，得到：(c1-c2) (f1-f2) =A，需要考虑c‘ 可逆，向量是短的等等，与LaBRADOR的投影有关（是relaxed的）。见Greyhound的lemma 3.2：CWSS：Commit-With-Small-Solution，保证协议“要么提取有效见证，要么破坏格假设”。
